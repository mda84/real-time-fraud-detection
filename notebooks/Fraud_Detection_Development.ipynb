{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Fraud Detection Pipeline\n",
    "\n",
    "This notebook demonstrates the end-to-end pipeline for a real-time fraud detection system. It covers:\n",
    "1. Data Ingestion: Download and load raw transaction data from Kaggle.\n",
    "2. Data Transformation: Clean the data and add additional features.\n",
    "3. Model Training: Train a fraud detection model (Logistic Regression in this example).\n",
    "4. Model Evaluation: Evaluate the model's performance on test data.\n",
    "5. Prediction: Run a sample prediction using the trained model.\n",
    "6. API Testing: Send HTTP requests to the FastAPI endpoints for prediction and history retrieval.\n",
    "\n",
    "Ensure that your environment is set up (e.g., with a fresh virtual environment and required packages installed) before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the parent directory (which contains data_pipeline, model, api, etc.) to the Python path.\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data preview:\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ingest the raw data (downloads if not already available)\n",
    "from data_pipeline.ingest import ingest_data\n",
    "\n",
    "df_raw = ingest_data()\n",
    "print(\"Raw data preview:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to data/transformed_data.csv.\n",
      "Transformed data preview:\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V23       V24       V25       V26       V27  \\\n",
      "0  0.098698  0.363787  ... -0.110474  0.066928  0.128539 -0.189115  0.133558   \n",
      "1  0.085102 -0.255425  ...  0.101288 -0.339846  0.167170  0.125895 -0.008983   \n",
      "2  0.247676 -1.514654  ...  0.909412 -0.689281 -0.327642 -0.139097 -0.055353   \n",
      "3  0.377436 -1.387024  ... -0.190321 -1.175575  0.647376 -0.221929  0.062723   \n",
      "4 -0.270533  0.817739  ... -0.137458  0.141267 -0.206010  0.502292  0.219422   \n",
      "\n",
      "        V28  Amount  Class  Log_Amount  Time_norm  \n",
      "0 -0.021053  149.62      0    5.014760   0.000000  \n",
      "1  0.014724    2.69      0    1.305626   0.000000  \n",
      "2 -0.059752  378.66      0    5.939276   0.000006  \n",
      "3  0.061458  123.50      0    4.824306   0.000006  \n",
      "4  0.215153   69.99      0    4.262539   0.000012  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform the raw data and save the transformed CSV\n",
    "from data_pipeline.transform import transform_data\n",
    "\n",
    "input_csv = \"data/raw_transactions.csv\"\n",
    "output_csv = \"data/transformed_data.csv\"\n",
    "df_transformed = transform_data(input_csv, output_csv)\n",
    "print(\"Transformed data preview:\")\n",
    "print(df_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.83      0.61      0.71        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.81      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Model saved to model/fraud_model.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train the fraud detection model using the transformed data.\n",
    "from model.train import train_model\n",
    "\n",
    "trained_model = train_model(\"data/transformed_data.csv\")\n",
    "\n",
    "# The model is saved as 'model/fraud_model.pkl' after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284315\n",
      "           1       0.87      0.70      0.77       492\n",
      "\n",
      "    accuracy                           1.00    284807\n",
      "   macro avg       0.93      0.85      0.89    284807\n",
      "weighted avg       1.00      1.00      1.00    284807\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284315\n",
      "           1       0.87      0.70      0.77       492\n",
      "\n",
      "    accuracy                           1.00    284807\n",
      "   macro avg       0.93      0.85      0.89    284807\n",
      "weighted avg       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model.evaluate import evaluate_model\n",
    "\n",
    "# Evaluate the model on the transformed data (or use a separate test CSV if available)\n",
    "evaluation_report = evaluate_model(\"model/fraud_model.pkl\", \"data/transformed_data.csv\")\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Fraud Probability: 0.022274062758189307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model.predict import predict\n",
    "\n",
    "# Create a sample feature vector. Note: the model was trained on all columns except \"Class\".\n",
    "# You must supply a list of numeric values with the same length as the training features.\n",
    "# Here, we assume a dummy vector of zeros (adjust as necessary):\n",
    "sample_features = [0.0] * (len(df_transformed.columns) - 1)  # minus target column \"Class\"\n",
    "\n",
    "fraud_probability = predict(\"model/fraud_model.pkl\", sample_features)\n",
    "print(\"Sample Fraud Probability:\", fraud_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Base URL for the API (ensure the FastAPI server is running, e.g., via uvicorn api.app:app --host 0.0.0.0 --port 8000)\n",
    "API_URL = 'http://localhost:8000'\n",
    "API_KEY = 'secret-key'\n",
    "headers = {\"x-api-key\": API_KEY}\n",
    "\n",
    "print(\"Testing /predict endpoint...\")\n",
    "predict_endpoint = f\"{API_URL}/predict\"\n",
    "\n",
    "# Replace with actual features from your transformed data; here we use the same dummy vector\n",
    "payload = {\n",
    "    \"features\": sample_features,\n",
    "    \"transaction_id\": \"txn_001\"\n",
    "}\n",
    "\n",
    "response = requests.post(predict_endpoint, json=payload, headers=headers)\n",
    "print(\"Prediction API Response:\")\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "print(\"\\nTesting /history endpoint...\")\n",
    "history_endpoint = f\"{API_URL}/history/txn_001\"\n",
    "response_history = requests.get(history_endpoint, headers=headers)\n",
    "print(json.dumps(response_history.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
